{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from diffusers import UNet2DModel\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "from diffusers import DDPMScheduler\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_size = 32  # the generated image resolution\n",
    "    saved_model = \"\" #saved model path\n",
    "    class_num = 10\n",
    "    batch_size= 512\n",
    "    seed = 24\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(config.image_size),\n",
    "    transforms.CenterCrop(config.image_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the output of the mid block of the Unet model (backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NewModel(nn.Module):\n",
    "    def __init__(self, conv_in, time_proj, down_blocks, mid_block, dtype, time_embedding, config):\n",
    "        super(NewModel, self).__init__()\n",
    "        self.conv_in=conv_in\n",
    "        self.time_proj=time_proj\n",
    "        self.down_blocks = down_blocks\n",
    "        self.mid_block = mid_block\n",
    "        self.dtype=dtype\n",
    "        self.time_embedding=time_embedding\n",
    "        self.config=config\n",
    "\n",
    "    def forward(self, sample):\n",
    "        # 0. center input if necessary\n",
    "        if self.config.center_input_sample:\n",
    "            sample = 2 * sample - 1.0\n",
    "        timesteps = 0\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\n",
    "        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(sample.device)\n",
    "\n",
    "        timesteps = timesteps * torch.ones(sample.shape[0], dtype=timesteps.dtype, device=timesteps.device)\n",
    "        t_emb = self.time_proj(timesteps)\n",
    "        t_emb = t_emb.to(dtype=self.dtype)\n",
    "        emb = self.time_embedding(t_emb)\n",
    "\n",
    "\n",
    "        # print(\"time done\")\n",
    "        # 2. pre-process\n",
    "        skip_sample = sample\n",
    "        sample = self.conv_in(sample)\n",
    "        # print(\"pre-process done\")\n",
    "        # 3. down\n",
    "        down_block_res_samples = (sample,)\n",
    "        for downsample_block in self.down_blocks:\n",
    "            if hasattr(downsample_block, \"skip_conv\"):\n",
    "                sample, res_samples, skip_sample = downsample_block(\n",
    "                    hidden_states=sample, temb=emb, skip_sample=skip_sample\n",
    "                )\n",
    "            else:\n",
    "                sample, res_samples = downsample_block(hidden_states=sample, temb=emb)\n",
    "\n",
    "            down_block_res_samples += res_samples\n",
    "        # print(\"down done\")\n",
    "        # 4. mid\n",
    "        sample = self.mid_block(sample, emb)\n",
    "\n",
    "        # Return the output from the mid-block\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unet = UNet2DModel.from_pretrained(config.saved_model)\n",
    "AlteredUnetModel=NewModel(Unet.conv_in, Unet.time_proj, Unet.down_blocks,Unet.mid_block, Unet.dtype, Unet.time_embedding, Unet.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading CIFAR 10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "datasetCIFAR10 = datasets.CIFAR10(root='/artifacts/datasetcifar10train', train=True, download=True, transform=preprocess)\n",
    "datasetCIFAR10test = datasets.CIFAR10(root='/artifacts/datasetcifar10test', train=False, download=True, transform=preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = torch.utils.data.DataLoader(\n",
    "    datasetCIFAR10,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "dataloader_val = torch.utils.data.DataLoader(\n",
    "    datasetCIFAR10test,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-NN classification. \n",
    "\n",
    "This was not written by me, the source can be found here: https://github.com/lightly-ai/lightly/blob/master/lightly/utils/benchmarking/knn_classifier.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning import LightningModule\n",
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "\n",
    "from lightly.utils.benchmarking import knn_predict\n",
    "from lightly.utils.benchmarking.topk import mean_topk_accuracy\n",
    "\n",
    "\n",
    "class KNNClassifier(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Module,\n",
    "        num_classes: int,\n",
    "        knn_k: int = 200,\n",
    "        knn_t: float = 0.1,\n",
    "        topk: Tuple[int, ...] = (1, 5),\n",
    "        feature_dtype: torch.dtype = torch.float32,\n",
    "        normalize: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(\n",
    "            {\n",
    "                \"num_classes\": num_classes,\n",
    "                \"knn_k\": knn_k,\n",
    "                \"knn_t\": knn_t,\n",
    "                \"topk\": topk,\n",
    "                \"feature_dtype\": str(feature_dtype),\n",
    "            }\n",
    "        )\n",
    "        self.model = model\n",
    "        self.num_classes = num_classes\n",
    "        self.knn_k = knn_k\n",
    "        self.knn_t = knn_t\n",
    "        self.topk = topk\n",
    "        self.feature_dtype = feature_dtype\n",
    "        self.normalize = normalize\n",
    "\n",
    "        self._train_features = []\n",
    "        self._train_targets = []\n",
    "        self._train_features_tensor: Optional[Tensor] = None\n",
    "        self._train_targets_tensor: Optional[Tensor] = None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def training_step(self, batch, batch_idx) -> None:\n",
    "        images, targets = batch[0], batch[1]\n",
    "        features = self.model.forward(images).flatten(start_dim=1)\n",
    "        if self.normalize:\n",
    "            features = F.normalize(features, dim=1)\n",
    "        features = features.to(self.feature_dtype)\n",
    "        self._train_features.append(features.cpu())\n",
    "        self._train_targets.append(targets.cpu())\n",
    "\n",
    "    def validation_step(self, batch, batch_idx) -> None:\n",
    "        if self._train_features_tensor is None or self._train_targets_tensor is None:\n",
    "            return\n",
    "\n",
    "        images, targets = batch[0], batch[1]\n",
    "        features = self.model.forward(images).flatten(start_dim=1)\n",
    "        if self.normalize:\n",
    "            features = F.normalize(features, dim=1)\n",
    "        features = features.to(self.feature_dtype)\n",
    "        predicted_classes = knn_predict(\n",
    "            feature=features,\n",
    "            feature_bank=self._train_features_tensor,\n",
    "            feature_labels=self._train_targets_tensor,\n",
    "            num_classes=self.num_classes,\n",
    "            knn_k=self.knn_k,\n",
    "            knn_t=self.knn_t,\n",
    "        )\n",
    "        topk = mean_topk_accuracy(\n",
    "            predicted_classes=predicted_classes, targets=targets, k=self.topk\n",
    "        )\n",
    "        log_dict = {f\"val_top{k}\": acc for k, acc in topk.items()}\n",
    "        self.log_dict(log_dict, prog_bar=True, sync_dist=True, batch_size=len(targets))\n",
    "\n",
    "    def on_validation_epoch_start(self) -> None:\n",
    "        if self._train_features and self._train_targets:\n",
    "            # Features and targets have size (world_size, batch_size, dim) and\n",
    "            # (world_size, batch_size) after gather. For non-distributed training,\n",
    "            # features and targets have size (batch_size, dim) and (batch_size,).\n",
    "            features = self.all_gather(torch.cat(self._train_features, dim=0))\n",
    "            self._train_features = []\n",
    "            targets = self.all_gather(torch.cat(self._train_targets, dim=0))\n",
    "            self._train_targets = []\n",
    "            # Reshape to (dim, world_size * batch_size)\n",
    "            features = features.flatten(end_dim=-2).t().contiguous()\n",
    "            self._train_features_tensor = features.to(self.device)\n",
    "            # Reshape to (world_size * batch_size,)\n",
    "            targets = targets.flatten().t().contiguous()\n",
    "            self._train_targets_tensor = targets.to(self.device)\n",
    "\n",
    "    def on_train_epoch_start(self) -> None:\n",
    "        # Set model to eval mode to disable norm layer updates.\n",
    "        self.model.eval()\n",
    "\n",
    "        # Reset features and targets.\n",
    "        self._train_features = []\n",
    "        self._train_targets = []\n",
    "        self._train_features_tensor = None\n",
    "        self._train_targets_tensor = None\n",
    "\n",
    "    def configure_optimizers(self) -> None:\n",
    "        # configure_optimizers must be implemented for PyTorch Lightning. Returning None\n",
    "        # means that no optimization is performed.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_classifier = KNNClassifier(\n",
    "    model=AlteredUnetModel,\n",
    "    num_classes=10, # Assuming you have 10 classes for CIFAR10\n",
    "    knn_k=50,\n",
    "    knn_t=0.1,\n",
    "    topk=(1,2,3),\n",
    "    feature_dtype=torch.float32,\n",
    "    normalize=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "# Hardcode the wandb API key\n",
    "os.environ[\"WANDB_API_KEY\"] = \"\"   #Your wandb key\n",
    "\n",
    "\n",
    "wandb_logger = pl.loggers.WandbLogger(\n",
    "    name=\"\", project=\"\"    #Your run name and project name\n",
    ")\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=2, devices=1, accelerator=\"cuda\", logger=[wandb_logger]\n",
    ")\n",
    "trainer.fit(knn_classifier, dataloader_train, dataloader_val)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) CUDA_VISIBLE_DEVICES=1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
