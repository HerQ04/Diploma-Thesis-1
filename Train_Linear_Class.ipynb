{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_size = 64  # the generated image resolution\n",
    "    saved_model = \"/artifacts/cifar10_32pipeline/1599/unet\"\n",
    "    class_num = 102\n",
    "    batch_size= 256\n",
    "    seed = 24\n",
    "\n",
    "\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "preprocessTrain = transforms.Compose([\n",
    "    transforms.Resize(config.image_size),\n",
    "    transforms.CenterCrop(config.image_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "preprocessVal = transforms.Compose([\n",
    "    transforms.Resize(config.image_size),\n",
    "    transforms.CenterCrop(config.image_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DModel\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "from diffusers import DDPMScheduler\n",
    "\n",
    "model = UNet2DModel(\n",
    "    sample_size=config.image_size,  # the target image resolution\n",
    "    in_channels=3,  # the number of input channels, 3 for RGB images\n",
    "    out_channels=3,  # the number of output channels\n",
    "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "    block_out_channels=(128, 256, 256, 256),  # the number of output channels for each UNet block\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "        \"UpBlock2D\",\n",
    "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting only the Encoder and mid part from the U-net (Backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class NewModel(nn.Module):\n",
    "    def __init__(self, conv_in, time_proj, down_blocks, mid_block, dtype, time_embedding, config):\n",
    "        super(NewModel, self).__init__()\n",
    "        self.conv_in=conv_in\n",
    "        self.time_proj=time_proj\n",
    "        self.down_blocks = down_blocks\n",
    "        self.mid_block = mid_block\n",
    "        self.dtype=dtype\n",
    "        self.time_embedding=time_embedding\n",
    "        self.config=config\n",
    "\n",
    "    def forward(self, sample):\n",
    "        # 0. center input if necessary\n",
    "        if self.config.center_input_sample:\n",
    "            sample = 2 * sample - 1.0\n",
    "        timesteps = 0\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\n",
    "        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(sample.device)\n",
    "\n",
    "        timesteps = timesteps * torch.ones(sample.shape[0], dtype=timesteps.dtype, device=timesteps.device)\n",
    "        t_emb = self.time_proj(timesteps)\n",
    "        t_emb = t_emb.to(dtype=self.dtype)\n",
    "        emb = self.time_embedding(t_emb)\n",
    "\n",
    "\n",
    "        # print(\"time done\")\n",
    "        # 2. pre-process\n",
    "        skip_sample = sample\n",
    "        sample = self.conv_in(sample)\n",
    "        # print(\"pre-process done\")\n",
    "        # 3. down\n",
    "        down_block_res_samples = (sample,)\n",
    "        for downsample_block in self.down_blocks:\n",
    "            if hasattr(downsample_block, \"skip_conv\"):\n",
    "                sample, res_samples, skip_sample = downsample_block(\n",
    "                    hidden_states=sample, temb=emb, skip_sample=skip_sample\n",
    "                )\n",
    "            else:\n",
    "                sample, res_samples = downsample_block(hidden_states=sample, temb=emb)\n",
    "\n",
    "            down_block_res_samples += res_samples\n",
    "        # print(\"down done\")\n",
    "        # 4. mid\n",
    "        sample = self.mid_block(sample, emb)\n",
    "\n",
    "        # Return the output from the mid-block\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unet = UNet2DModel.from_pretrained(config.saved_model)\n",
    "AlteredUnetModel=NewModel(Unet.conv_in, Unet.time_proj, Unet.down_blocks,Unet.mid_block, Unet.dtype, Unet.time_embedding, Unet.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "import pytorch_lightning as pl\n",
    "from torch.nn import AdaptiveAvgPool2d\n",
    "global_avg_pool = AdaptiveAvgPool2d((1, 1))\n",
    "from lightly.transforms import utils\n",
    "\n",
    "class Classifier(pl.LightningModule):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        # use the pretrained backbone\n",
    "        self.backbone = backbone\n",
    "\n",
    "        # freeze the backbone\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # create a linear layer for our downstream classification model\n",
    "        self.fc = nn.Linear(256, config.class_num)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "        self.example_input_array = torch.zeros(1, 3, config.image_size, config.image_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "            mid_block_output = self.backbone(x)\n",
    "            pooled_output = global_avg_pool(mid_block_output)\n",
    "            # Flatten the output correctly\n",
    "            y_hat = pooled_output.view(pooled_output.size(0), -1)\n",
    "            y_hat = self.fc(y_hat)\n",
    "            return y_hat\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log(\"train_loss_fc\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        y_hat = torch.nn.functional.softmax(y_hat, dim=1)\n",
    "\n",
    "        # calculate number of correct predictions\n",
    "        _, predicted = torch.max(y_hat, 1)\n",
    "        num = predicted.shape[0]\n",
    "        correct = (predicted == y).float().sum()\n",
    "        self.validation_step_outputs.append((num, correct))\n",
    "        return num, correct\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # calculate and log top1 accuracy\n",
    "        if self.validation_step_outputs:\n",
    "            total_num = 0\n",
    "            total_correct = 0\n",
    "            for num, correct in self.validation_step_outputs:\n",
    "                total_num += num\n",
    "                total_correct += correct\n",
    "            acc = total_correct / total_num\n",
    "            self.log(\"val_acc\", acc, on_epoch=True, prog_bar=True)\n",
    "            self.validation_step_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.Adam(self.fc.parameters(), lr=0.001)\n",
    "        return optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "datasetCIFAR10 = datasets.CIFAR10(root='/artifacts/datasetcifar10train', train=True, download=True, transform=preprocessTrain)\n",
    "datasetCIFAR10test = datasets.CIFAR10(root='/artifacts/datasetcifar10test', train=False, download=True, transform=preprocessVal)\n",
    "datasetFlower = datasets.Flowers102(root='/artifacts/datasetflowerstrain', download=True, split=\"train\", transform=preprocessTrain)\n",
    "datasetFlowerval = datasets.Flowers102(root='/artifacts/datasetflowersval', download=True, split=\"val\", transform=preprocessVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = torch.utils.data.DataLoader(\n",
    "    datasetFlower,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "dataloader_val = torch.utils.data.DataLoader(\n",
    "    datasetFlowerval,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "# Hardcode the wandb API key\n",
    "os.environ[\"WANDB_API_KEY\"] = \"c4a1609343aa95e7766a99bfd520c821951bf638\"\n",
    "\n",
    "AlteredUnetModel.eval()\n",
    "classifier = Classifier(AlteredUnetModel)\n",
    "\n",
    "wandb_logger = pl.loggers.WandbLogger(\n",
    "    name=\"Docker64Flower4\", project=\"Backbone linear\"\n",
    ")\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=30, devices=1, accelerator=\"cuda\", logger=[wandb_logger]\n",
    ")\n",
    "trainer.fit(classifier, dataloader_train, dataloader_val)\n",
    "# trainer.fit(classifier, train_loader, val_loader)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) CUDA_VISIBLE_DEVICES=1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
